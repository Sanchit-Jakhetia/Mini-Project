# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TolwE_Cs8qnKGP063RPxuVUwNdBs9BKL

# Task
make a prediction model to predict sem3_gpa

Here is all the data you need:
"student_performance_data.csv"

## Data loading

### Subtask:
Load the student performance data from the provided CSV file.

**Reasoning**:
Load the data from the provided CSV file into a pandas DataFrame and display its first few rows and shape to verify the data has been loaded correctly.
"""

import pandas as pd

try:
    df = pd.read_csv('student_performance_data.csv')
    display(df.head())
    print(df.shape)
except FileNotFoundError:
    print("Error: 'student_performance_data.csv' not found. Please ensure the file exists in the current directory.")
except pd.errors.ParserError:
    print("Error: Could not parse the CSV file. Please check the file format.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

"""## Data exploration

### Subtask:
Explore the loaded data to understand its structure, identify missing values, and examine the distribution of variables, particularly 'sem3_gpa'.

**Reasoning**:
I need to explore the data, identify missing values, examine data types, and calculate descriptive statistics, especially for 'sem3_gpa'.  I will also look at the distributions of numerical features and explore categorical variables.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Missing Values
print("Missing Values:\n", df.isnull().sum())
missing_percentage = (df.isnull().sum() / len(df)) * 100
print("\nMissing Value Percentage:\n", missing_percentage)

plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

# Data Types
print("\nData Types:\n", df.dtypes)

# Descriptive Statistics
print("\nDescriptive Statistics:\n", df.describe())
plt.figure(figsize=(8, 6))
sns.histplot(df['semester_3_gpa'], kde=True)
plt.title('Distribution of Semester 3 GPA')
plt.show()

# Numerical Feature Distributions
numerical_features = df.select_dtypes(include=['number'])
for col in numerical_features.columns:
    plt.figure(figsize=(8, 6))
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()

# Categorical Variable Exploration
categorical_features = df.select_dtypes(include=['object'])
for col in categorical_features.columns:
    print(f'\nValue counts for {col}:\n{df[col].value_counts()}')
    plt.figure(figsize=(8, 6))
    df[col].value_counts().plot(kind='bar')
    plt.title(f'Distribution of {col}')
    plt.show()

# Correlation Analysis
correlation_matrix = df.corr()
plt.figure(figsize=(14, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Outlier Detection (using boxplots for numerical features)
for col in numerical_features.columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')
    plt.show()

"""**Reasoning**:
The previous code block failed due to a `ValueError` when calculating the correlation matrix because some columns contain non-numeric data. I will handle this by explicitly specifying `numeric_only=True` in the `.corr()` method.  Additionally, I will remove the error-causing line in the outlier detection section.


"""

import matplotlib.pyplot as plt
import seaborn as sns

# Missing Values
print("Missing Values:\n", df.isnull().sum())
missing_percentage = (df.isnull().sum() / len(df)) * 100
print("\nMissing Value Percentage:\n", missing_percentage)

plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

# Data Types
print("\nData Types:\n", df.dtypes)

# Descriptive Statistics
print("\nDescriptive Statistics:\n", df.describe(include='all')) # Include all columns
plt.figure(figsize=(8, 6))
sns.histplot(df['semester_3_gpa'], kde=True)
plt.title('Distribution of Semester 3 GPA')
plt.show()

# Numerical Feature Distributions
numerical_features = df.select_dtypes(include=['number'])
for col in numerical_features.columns:
    plt.figure(figsize=(8, 6))
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()

# Categorical Variable Exploration
categorical_features = df.select_dtypes(include=['object'])
for col in categorical_features.columns:
    print(f'\nValue counts for {col}:\n{df[col].value_counts()}')
    plt.figure(figsize=(8, 6))
    df[col].value_counts().plot(kind='bar')
    plt.title(f'Distribution of {col}')
    plt.show()

# Correlation Analysis (handling non-numeric columns)
correlation_matrix = df.corr(numeric_only=True)
plt.figure(figsize=(14, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Outlier Detection (boxplots for numerical features)
for col in numerical_features.columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')
    plt.show()

"""## Data cleaning

### Subtask:
Clean the data by handling missing values, outliers, and duplicates.

**Reasoning**:
Impute missing values in 'extracurricular_focus' with the mode, handle outliers in numerical features using IQR, and remove duplicate rows. Then, verify the cleaning process.
"""

# Impute missing values in 'extracurricular_focus' with the mode
df['extracurricular_focus'].fillna(df['extracurricular_focus'].mode()[0], inplace=True)

# Handle outliers in numerical features using IQR
numerical_cols = ['10th_percentage', '12th_percentage', 'math_score_10th', 'science_score_10th', 'average_study_hours_per_day', 'attendance_10th', 'attendance_12th', 'semester_1_gpa', 'semester_2_gpa', 'semester_3_gpa', 'average_attendance_semesters', 'average_interest_level']
for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

# Remove duplicate rows
df.drop_duplicates(inplace=True)

# Verify data cleaning
print("Missing values after imputation:\n", df.isnull().sum())
print("\nSummary statistics after outlier handling:\n", df.describe())
for col in numerical_cols:
    plt.figure(figsize=(8, 6))
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col} after outlier handling')
    plt.show()

"""**Reasoning**:
Fix the warning from the previous code block and regenerate the code block.


"""

# Impute missing values in 'extracurricular_focus' with the mode
df['extracurricular_focus'] = df['extracurricular_focus'].fillna(df['extracurricular_focus'].mode()[0])

# Handle outliers in numerical features using IQR
numerical_cols = ['10th_percentage', '12th_percentage', 'math_score_10th', 'science_score_10th', 'average_study_hours_per_day', 'attendance_10th', 'attendance_12th', 'semester_1_gpa', 'semester_2_gpa', 'semester_3_gpa', 'average_attendance_semesters', 'average_interest_level']
for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

# Remove duplicate rows
df.drop_duplicates(inplace=True)

# Verify data cleaning
print("Missing values after imputation:\n", df.isnull().sum())
print("\nSummary statistics after outlier handling:\n", df.describe())
for col in numerical_cols:
    plt.figure(figsize=(8, 6))
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col} after outlier handling')
    plt.show()

"""## Feature engineering

### Subtask:
Engineer new features from the existing data to improve model performance.

**Reasoning**:
Create interaction terms, transform existing features (including one-hot encoding), generate polynomial features, and then analyze the correlation matrix of the new features with the target variable.
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import PolynomialFeatures

# Interaction terms
df['interaction_10th_12th_percentage'] = df['10th_percentage'] * df['12th_percentage']
df['interaction_study_hours_attendance'] = df['average_study_hours_per_day'] * df['average_attendance_semesters']
df['interaction_sem1_sem2_gpa'] = df['semester_1_gpa'] * df['semester_2_gpa']


# Transform existing features
df['high_sem1_gpa'] = (df['semester_1_gpa'] > 6).astype(int)
df['high_sem2_gpa'] = (df['semester_2_gpa'] > 6).astype(int)

# Binning average study hours
bins = [0, 2, 4, 6, float('inf')]
labels = ['Low', 'Medium', 'High', 'Very High']
df['study_hours_binned'] = pd.cut(df['average_study_hours_per_day'], bins=bins, labels=labels, right=False)

# One-hot encode categorical features
categorical_cols = ['study_method', 'preferred_learning_style', 'extracurricular_focus', 'parental_education_level', 'study_hours_binned']
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)
numerical_cols = ['10th_percentage', '12th_percentage', 'average_study_hours_per_day', 'semester_1_gpa', 'semester_2_gpa']
poly_features = poly.fit_transform(df[numerical_cols])
poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(numerical_cols))
df = pd.concat([df, poly_df], axis=1)


# Correlation analysis with the new features
correlation_matrix = df.corr(numeric_only=True)
plt.figure(figsize=(20, 16))
sns.heatmap(correlation_matrix[['semester_3_gpa']], annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix with semester_3_gpa')
plt.show()

"""## Data splitting

### Subtask:
Split the data into training, validation, and testing sets.

**Reasoning**:
Split the data into training, validation, and testing sets using train_test_split from scikit-learn.  Stratify the split based on 'performance_label' to maintain class distribution.
"""

from sklearn.model_selection import train_test_split

# Separate features (X) and target variable (y)
X = df.drop('semester_3_gpa', axis=1)
y = df['semester_3_gpa']

# Split data into training (70%), validation (15%), and testing (15%) sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=df['performance_label'])
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=X_temp['performance_label'])

# Print shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""## Model training

### Subtask:
Train Regression Models

**Reasoning**:
Train the specified regression models on the training data.
"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

# Initialize models
linear_model = LinearRegression()
tree_model = DecisionTreeRegressor(random_state=42)
forest_model = RandomForestRegressor(random_state=42)

# Train models
linear_model.fit(X_train, y_train)
tree_model.fit(X_train, y_train)
forest_model.fit(X_train, y_train)

"""**Reasoning**:
The error message indicates that there are non-numeric values in the dataset, which are causing issues with the model training.  I will examine the data and handle the non-numeric values before retraining the models.  It looks like the error occurs during the `astype` call, so let's look at the `X_train` data.


"""

display(X_train.head())
display(X_train.info())
display(X_train.describe(include='all'))

"""**Reasoning**:
The previous error indicates that the `performance_label` column contains non-numeric values.  Since it's a categorical feature, it needs to be one-hot encoded.  I will re-do the feature engineering step, correctly encoding the `performance_label` column this time, and then re-split the data before retraining the models.


"""

# One-hot encode 'performance_label'
categorical_cols = ['performance_label']
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Separate features (X) and target variable (y)
X = df.drop('semester_3_gpa', axis=1)
y = df['semester_3_gpa']

# Split data into training (70%), validation (15%), and testing (15%) sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Initialize models
linear_model = LinearRegression()
tree_model = DecisionTreeRegressor(random_state=42)
forest_model = RandomForestRegressor(random_state=42)

# Train models
linear_model.fit(X_train, y_train)
tree_model.fit(X_train, y_train)
forest_model.fit(X_train, y_train)

"""## Model optimization

### Subtask:
Optimize the hyperparameters of the trained regression models.

**Reasoning**:
Optimize the hyperparameters of the three regression models using GridSearchCV or RandomizedSearchCV.
"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Define parameter grids for each model
param_grid_linear = {}  # Linear Regression has limited hyperparameters

param_grid_tree = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

param_grid_forest = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Initialize models
linear_model = LinearRegression()
tree_model = DecisionTreeRegressor(random_state=42)
forest_model = RandomForestRegressor(random_state=42)

# Perform GridSearchCV or RandomizedSearchCV
grid_search_linear = GridSearchCV(linear_model, param_grid_linear, scoring='neg_mean_squared_error', cv=5)
grid_search_linear.fit(X_train, y_train)
print(f"Best hyperparameters for Linear Regression: {grid_search_linear.best_params_}")
print(f"Best score for Linear Regression: {-grid_search_linear.best_score_}")

grid_search_tree = GridSearchCV(tree_model, param_grid_tree, scoring='neg_mean_squared_error', cv=5)
grid_search_tree.fit(X_train, y_train)
print(f"Best hyperparameters for Decision Tree: {grid_search_tree.best_params_}")
print(f"Best score for Decision Tree: {-grid_search_tree.best_score_}")

random_search_forest = RandomizedSearchCV(forest_model, param_grid_forest, scoring='neg_mean_squared_error', n_iter=10, cv=5, random_state=42)
random_search_forest.fit(X_train, y_train)
print(f"Best hyperparameters for Random Forest: {random_search_forest.best_params_}")
print(f"Best score for Random Forest: {-random_search_forest.best_score_}")

# Retrain models with best hyperparameters
best_linear_model = grid_search_linear.best_estimator_
best_tree_model = grid_search_tree.best_estimator_
best_forest_model = random_search_forest.best_estimator_

# Combine train and validation sets
X_train_combined = pd.concat([X_train, X_val])
y_train_combined = pd.concat([y_train, y_val])

best_linear_model.fit(X_train_combined, y_train_combined)
best_tree_model.fit(X_train_combined, y_train_combined)
best_forest_model.fit(X_train_combined, y_train_combined)

"""## Model evaluation

### Subtask:
Evaluate the performance of the optimized regression models on the held-out test set.

**Reasoning**:
Evaluate the performance of the optimized models on the test set using MSE, RMSE, and R-squared, then print the results and discuss the findings.
"""

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pandas as pd

# Predict on the test set
y_pred_linear = best_linear_model.predict(X_test)
y_pred_tree = best_tree_model.predict(X_test)
y_pred_forest = best_forest_model.predict(X_test)

# Calculate evaluation metrics
mse_linear = mean_squared_error(y_test, y_pred_linear)
rmse_linear = np.sqrt(mse_linear)
r2_linear = r2_score(y_test, y_pred_linear)

mse_tree = mean_squared_error(y_test, y_pred_tree)
rmse_tree = np.sqrt(mse_tree)
r2_tree = r2_score(y_test, y_pred_tree)

mse_forest = mean_squared_error(y_test, y_pred_forest)
rmse_forest = np.sqrt(mse_forest)
r2_forest = r2_score(y_test, y_pred_forest)

# Create a DataFrame for clear comparison
results_df = pd.DataFrame({
    'Model': ['Linear Regression', 'Decision Tree', 'Random Forest'],
    'MSE': [mse_linear, mse_tree, mse_forest],
    'RMSE': [rmse_linear, rmse_tree, rmse_forest],
    'R-squared': [r2_linear, r2_tree, r2_forest]
})

display(results_df)

# Discuss model performance
print("\nModel Performance Discussion:")
print("Based on the evaluation metrics, the Random Forest model appears to be the best performer, with the lowest RMSE and highest R-squared among the three models.")
print("However, a high R-squared doesn't always guarantee the best predictive performance. The model's performance on unseen data should be further validated with robust testing.")

"""## Summary:

### Q&A
No questions to answer.

### Data Analysis Key Findings
* **Missing Data Imputation:** Missing values in the 'extracurricular\_focus' column (24.6\\%) were imputed using the mode.
* **Outlier Handling:** Outliers in several numerical features were handled using the IQR method.
* **Feature Engineering:** New features were created through interaction terms, transformations (e.g., indicator variables, binning), and polynomial features.  The interaction of 10th and 12th percentage, study hours and attendance, and semester 1 and 2 GPAs were created as new features.
* **Model Performance:**  The Random Forest model demonstrated the best performance on the test set, with the lowest RMSE (1.209) and a reasonable R-squared (0.509) compared to Linear Regression and Decision Tree models.  However, further validation on unseen data is recommended.
* **Hyperparameter Optimization:**  Hyperparameters for the Decision Tree and Random Forest models were optimized using GridSearchCV and RandomizedSearchCV, respectively. The best hyperparameters are reported for each model.

### Insights or Next Steps
* Evaluate the models with more robust testing on unseen data to confirm their generalizability.
* Explore other regression models or ensemble methods to potentially improve predictive accuracy.

"""